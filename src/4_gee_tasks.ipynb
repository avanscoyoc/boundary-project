{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203e1689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: NDBI - Normalized Difference Built-up Index\n",
      "Google Drive folder: ndbi_raw\n",
      "After download, place CSVs in: ../results/ndbi/\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for utils import\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "from utils import INDEX_CONFIGS, make_gradient\n",
    "\n",
    "ee.Authenticate()\n",
    "ee.Initialize(project='dse-staff')\n",
    "\n",
    "# ===== CONFIGURATION: SELECT INDEX =====\n",
    "INDEX_NAME = 'ndbi'  # Change to: 'ndvi', 'ndbi', 'lai', or 'fpar'\n",
    "# =======================================\n",
    "\n",
    "# Validate index selection\n",
    "if INDEX_NAME not in INDEX_CONFIGS:\n",
    "    raise ValueError(f\"Invalid index: {INDEX_NAME}. Choose from {list(INDEX_CONFIGS.keys())}\")\n",
    "\n",
    "# Folder configuration - exports to Google Drive folder, download to results/{INDEX_NAME}/\n",
    "folder_name = f\"{INDEX_NAME}_raw\"\n",
    "print(f\"Processing: {INDEX_NAME.upper()} - {INDEX_CONFIGS[INDEX_NAME]['description']}\")\n",
    "print(f\"Google Drive folder: {folder_name}\")\n",
    "print(f\"After download, place CSVs in: ../results/{INDEX_NAME}/\")\n",
    "\n",
    "# Static layers (unchanged)\n",
    "gsw = ee.Image('JRC/GSW1_4/GlobalSurfaceWater')\n",
    "hm = ee.ImageCollection('CSP/HM/GlobalHumanModification').mean()\n",
    "elevation = ee.Image('USGS/SRTMGL1_003').select('elevation')\n",
    "slope = ee.Terrain.slope(elevation)\n",
    "\n",
    "staticImage = ee.Image.cat([\n",
    "    gsw.select('max_extent'),\n",
    "    hm.rename('gHM'),\n",
    "    elevation,\n",
    "    slope\n",
    "])\n",
    "\n",
    "# Year configuration\n",
    "years = ee.List.sequence(2003, 2025)\n",
    "gradBandNames = [str(y) for y in range(2003, 2026)]\n",
    "selectors = ['WDPA_PID', 'transectID', 'pointID', 'max_extent', 'gHM', 'elevation', 'slope'] + gradBandNames\n",
    "\n",
    "# Build gradient function for selected index\n",
    "def make_current_gradient(y):\n",
    "    return make_gradient(INDEX_NAME, y)\n",
    "\n",
    "# Build image with gradient bands (your existing logic)\n",
    "gradientBands = ee.ImageCollection.fromImages(\n",
    "    years.map(make_current_gradient)\n",
    ").toBands()\n",
    "gradientBands = gradientBands.rename(gradBandNames)\n",
    "image = staticImage.addBands(gradientBands)\n",
    "\n",
    "\n",
    "def process_samples(asset_path, chunk_size=50_000, batch_size=10, chunks_to_run=None):\n",
    "    \"\"\"\n",
    "    Process samples from Earth Engine asset and export to Google Drive.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    asset_path : str\n",
    "        Path to Earth Engine FeatureCollection asset\n",
    "    chunk_size : int\n",
    "        Number of samples per chunk\n",
    "    batch_size : int\n",
    "        Number of chunks to process simultaneously\n",
    "    chunks_to_run : list, optional\n",
    "        Specific chunk indices to process (for rerunning failures)\n",
    "    \"\"\"\n",
    "    samples = ee.FeatureCollection(asset_path)\n",
    "    size = samples.size().getInfo()\n",
    "    nChunks = int((size + chunk_size - 1) // chunk_size)\n",
    "    tasks = []\n",
    "    \n",
    "    # If chunks_to_run is None, run all chunks\n",
    "    if chunks_to_run is None:\n",
    "        chunks_to_run = list(range(nChunks))\n",
    "    \n",
    "    # Extract asset number from path (e.g., \"chunk_003\" -> \"003\")\n",
    "    asset_num = asset_path.split(\"_\")[-1]\n",
    "    \n",
    "    # Create tasks only for specified chunks\n",
    "    for i in chunks_to_run:\n",
    "        fcChunk = ee.FeatureCollection(samples.toList(chunk_size, i * chunk_size))\n",
    "        sampled = image.reduceRegions(\n",
    "            collection=fcChunk,\n",
    "            reducer=ee.Reducer.first(),\n",
    "            scale=500\n",
    "        )\n",
    "        task = ee.batch.Export.table.toDrive(\n",
    "            collection=sampled,\n",
    "            description=f'{INDEX_NAME}_raw_grad_{asset_num}_chunk_{i}',\n",
    "            fileFormat='CSV',\n",
    "            selectors=selectors, \n",
    "            folder=folder_name\n",
    "        )\n",
    "        tasks.append((i, task))\n",
    "\n",
    "    # Process in batches\n",
    "    for j in range(0, len(tasks), batch_size):\n",
    "        batch = tasks[j:j + batch_size]\n",
    "        for idx, t in batch:\n",
    "            t.start()\n",
    "        \n",
    "        chunk_nums = [idx for idx, _ in batch]\n",
    "        print(f\"  Processing chunks {chunk_nums}...\")\n",
    "        \n",
    "        while True:\n",
    "            statuses = [t.status()['state'] for _, t in batch]\n",
    "            if all(s in ['COMPLETED', 'FAILED', 'CANCELLED'] for s in statuses):\n",
    "                print(f\"  Completed chunks {chunk_nums}\")\n",
    "                break\n",
    "            time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16138a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing asset 1 of 10: projects/dse-staff/assets/chunk_000\n",
      "  Processing chunks [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]...\n"
     ]
    }
   ],
   "source": [
    "# Process all assets sequentially #305 minutes\n",
    "total_assets = 10\n",
    "for idx in range(total_assets):\n",
    "    asset = f'projects/dse-staff/assets/chunk_{idx:03d}'\n",
    "    print(f\"\\nProcessing asset {idx + 1} of {total_assets}: {asset}\")\n",
    "    process_samples(asset)\n",
    "    print(f\"Asset {idx + 1} of {total_assets} complete\")\n",
    "\n",
    "print(f\"\\nAll {total_assets} assets processed for {INDEX_NAME.upper()}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ad947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprocess only failed chunks\n",
    "# Update the dictionary below with your failed chunks\n",
    "failed_by_asset = {\n",
    "    # 0: [14],      # Example: chunk_000 had chunk 14 fail\n",
    "    # 1: [9],       # chunk_001 had chunk 9 fail\n",
    "    # 2: [1],       # etc.\n",
    "    # 3: [4, 7]\n",
    "}\n",
    "\n",
    "if failed_by_asset:\n",
    "    for asset_idx, failed_chunks in failed_by_asset.items():\n",
    "        asset = f'projects/dse-staff/assets/chunk_{asset_idx:03d}'\n",
    "        print(f\"\\nReprocessing failed chunks {failed_chunks} for asset {asset_idx} ({asset})\")\n",
    "        process_samples(asset, chunks_to_run=failed_chunks)\n",
    "        print(f\"Asset {asset_idx} failed chunks reprocessed!\")\n",
    "    print(f\"\\nAll failed chunks reprocessed for {INDEX_NAME.upper()}!\")\n",
    "else:\n",
    "    print(\"No failed chunks specified. Update the failed_by_asset dictionary above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
