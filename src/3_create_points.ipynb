{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b606cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from utils import create_transects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ba2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdpa_filtered = gpd.read_file(\"../data/wdpa_filtered/wdpa_filtered.shp\")\n",
    "crs = wdpa_filtered.crs\n",
    "len(wdpa_filtered)  # 5012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9644a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters \n",
    "sample_dist = 500  # transect spacing (meters)\n",
    "transect_unit = 2500  # distance between samples along a transect (meters)\n",
    "transect_pts = 2  # number of points on each side of boundary point\n",
    "buffer_dist = transect_unit * transect_pts + 500  # size of inner buffer, evaluates point validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all interior buffers at once\n",
    "print(\"Creating inner buffers for all protected areas...\")\n",
    "wdpa_buffers = wdpa_filtered[['WDPA_PID', 'geometry']].copy()\n",
    "wdpa_buffers['geometry'] = wdpa_buffers.geometry.buffer(-buffer_dist)\n",
    "wdpa_buffer_dict = dict(zip(wdpa_buffers['WDPA_PID'], wdpa_buffers['geometry']))\n",
    "del wdpa_buffers  # Free memory - only need the dictionary\n",
    "\n",
    "# Diagnose any buffer issues\n",
    "print(\"Checking buffer validity...\")\n",
    "empty_buffers = sum(1 for geom in wdpa_buffer_dict.values() if geom.is_empty)\n",
    "invalid_buffers = sum(1 for geom in wdpa_buffer_dict.values() if not geom.is_valid)\n",
    "print(f\"  Total buffers created: {len(wdpa_buffer_dict)}\")\n",
    "print(f\"  Empty buffers (PA too small): {empty_buffers}\")\n",
    "print(f\"  Invalid buffers: {invalid_buffers}\")\n",
    "print(f\"  Valid non-empty buffers: {len(wdpa_buffer_dict) - empty_buffers - invalid_buffers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process PAs one at a time, filter immediately, write to chunks\n",
    "output_dir = \"../data/transect_chunks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processing {len(wdpa_filtered)} protected areas with streaming filter...\")\n",
    "chunk_files = []\n",
    "chunk_num = 0\n",
    "chunk_data = []\n",
    "total_points = 0\n",
    "total_transects = 0\n",
    "pas_processed = 0\n",
    "\n",
    "# Diagnostic counters\n",
    "no_transects = 0\n",
    "no_buffer = 0\n",
    "empty_buffer = 0\n",
    "all_filtered = 0\n",
    "\n",
    "# This function keeps transects for parks that pass these checks\n",
    "#✅ Transects generated successfully\n",
    "#✅ Generates an inner buffer in the dictionary\n",
    "#✅ Inner buffer is not empty\n",
    "#✅ Inner points are NOT inside the inner buffer\n",
    "#✅ Inner points are NOT outside the PA polygon\n",
    "#✅ Still has at least 1 transect remaining after filtering\n",
    "\n",
    "for idx, (_, park_row) in enumerate(wdpa_filtered.iterrows()):\n",
    "    # Generate transects for single PA\n",
    "    transect_df = create_transects((idx, park_row), sample_dist, transect_unit, transect_pts)\n",
    "    \n",
    "    if transect_df is None:\n",
    "        no_transects += 1\n",
    "        continue\n",
    "    \n",
    "    # Filter bad transects immediately for this PA\n",
    "    pid = park_row['WDPA_PID']\n",
    "    if pid not in wdpa_buffer_dict:\n",
    "        no_buffer += 1\n",
    "        continue\n",
    "    \n",
    "    # Check if buffer is empty, skip adding transects (PA too small)\n",
    "    buffer_geom = wdpa_buffer_dict[pid]\n",
    "    if buffer_geom.is_empty:\n",
    "        empty_buffer += 1\n",
    "        continue\n",
    "    \n",
    "    # Get inner points only\n",
    "    inner_pts = transect_df[transect_df['point_position'] < 0].copy()\n",
    "    \n",
    "    if len(inner_pts) > 0:\n",
    "        # Create minimal geodataframe for spatial check\n",
    "        inner_gdf = gpd.GeoDataFrame(\n",
    "            inner_pts[['WDPA_PID', 'transectID']],\n",
    "            geometry=gpd.points_from_xy(inner_pts['x'], inner_pts['y']),\n",
    "            crs=crs\n",
    "        )\n",
    "        \n",
    "        # Get the PA geometry\n",
    "        pa_geom = park_row.geometry\n",
    "        \n",
    "        # Mark inner points inside the inner buffer as bad (bad angle?)\n",
    "        bad_inside_buffer = inner_gdf[inner_gdf.geometry.within(buffer_geom)]['transectID'].unique()\n",
    "        \n",
    "        # Mark inner points outside the PA polygon as bad (crossed to opposite side)\n",
    "        bad_outside_pa = inner_gdf[~inner_gdf.geometry.within(pa_geom)]['transectID'].unique()\n",
    "        \n",
    "        # Combine both sets of bad transects\n",
    "        bad_transects = np.unique(np.concatenate([bad_inside_buffer, bad_outside_pa]))\n",
    "        \n",
    "        # Filter full data\n",
    "        transect_df = transect_df[~transect_df['transectID'].isin(bad_transects)]\n",
    "    \n",
    "    # If there are any transects left after filtering, add them to the chunk data\n",
    "    if len(transect_df) > 0:\n",
    "        total_points += len(transect_df)\n",
    "        total_transects += transect_df['transectID'].nunique()\n",
    "        pas_processed += 1\n",
    "        chunk_data.append(transect_df)\n",
    "    else:\n",
    "        all_filtered += 1\n",
    "    \n",
    "    # Write every 500 PAs\n",
    "    if len(chunk_data) >= 500:\n",
    "        chunk_file = f\"{output_dir}/chunk_{chunk_num:03d}.csv\"\n",
    "        pd.concat(chunk_data, ignore_index=True).to_csv(chunk_file, index=False)\n",
    "        chunk_files.append(chunk_file)\n",
    "        chunk_data = []\n",
    "        chunk_num += 1\n",
    "        gc.collect()\n",
    "        print(f\"  Processed {idx + 1}/{len(wdpa_filtered)} PAs, wrote chunk {chunk_num} | Total points: {total_points:,}\")\n",
    "\n",
    "# Write final chunk\n",
    "if chunk_data:\n",
    "    chunk_file = f\"{output_dir}/chunk_{chunk_num:03d}.csv\"\n",
    "    pd.concat(chunk_data, ignore_index=True).to_csv(chunk_file, index=False)\n",
    "    chunk_files.append(chunk_file)\n",
    "    print(f\"  Wrote final chunk {chunk_num + 1}\")\n",
    "\n",
    "# Clean up memory\n",
    "del wdpa_filtered, wdpa_buffer_dict, chunk_data\n",
    "gc.collect()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Processing Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total PAs in dataset: {idx + 1:,}\")\n",
    "print(f\"PAs with no transects generated: {no_transects:,}\")\n",
    "print(f\"PAs with no buffer: {no_buffer:,}\")\n",
    "print(f\"PAs with empty buffer (too small): {empty_buffer:,}\")\n",
    "print(f\"PAs with all transects filtered out: {all_filtered:,}\")\n",
    "print(f\"PAs successfully processed: {pas_processed:,}\")\n",
    "print(f\"-\" * 60)\n",
    "print(f\"Total transect points (filtered): {total_points:,}\")\n",
    "print(f\"Total unique transects: {total_transects:,}\")\n",
    "print(f\"Average transects per PA: {total_transects / pas_processed:.1f}\")\n",
    "print(f\"Average points per transect: {total_points / total_transects:.1f}\")\n",
    "print(f\"Created {len(chunk_files)} chunk files in {output_dir}/\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad12d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total PAs in dataset: 5,012\n",
    "#PAs with no transects generated: 0\n",
    "#PAs with no buffer: 0\n",
    "#PAs with empty buffer (too small): 753\n",
    "#PAs with all transects filtered out: 0\n",
    "#PAs successfully processed: 4,259\n",
    "#Total transect points (filtered): 9,029,755\n",
    "#Total unique transects: 1,805,951\n",
    "#Average transects per PA: 424.0\n",
    "#Average points per transect: 5.0\n",
    "#Created 9 chunk files in ../data/transect_chunks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6805f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform each chunk CRS from ESRI:54009 to EPSG:4326 for earth engine\n",
    "for chunk_file in sorted(glob.glob('../data/transect_chunks/chunk_*.csv')):\n",
    "    chunk = pd.read_csv(chunk_file, low_memory=False)\n",
    "    chunk_gdf = gpd.GeoDataFrame(chunk, geometry=gpd.points_from_xy(chunk['x'], chunk['y']), crs='ESRI:54009')\n",
    "    chunk_gdf = chunk_gdf.to_crs('EPSG:4326')\n",
    "    chunk['x'] = chunk_gdf.geometry.x\n",
    "    chunk['y'] = chunk_gdf.geometry.y\n",
    "    chunk.to_csv(chunk_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5fdbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 chunk files\n",
      "Writing combined file...\n",
      "  Wrote chunk 1/9\n",
      "  Wrote chunk 2/9\n",
      "  Wrote chunk 3/9\n",
      "  Wrote chunk 4/9\n",
      "  Wrote chunk 5/9\n",
      "  Wrote chunk 6/9\n",
      "  Wrote chunk 7/9\n",
      "  Wrote chunk 8/9\n",
      "  Wrote chunk 9/9\n",
      "\n",
      "Complete! Saved to ../data/all_transects_combined.csv\n",
      "File size: 4918.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Combine chunks, keeping only essential columns for Earth Engine\n",
    "chunk_files = sorted(glob.glob('../data/transect_chunks/chunk_*.csv'))\n",
    "print(f\"Found {len(chunk_files)} chunk files\")\n",
    "\n",
    "output_file = '../data/transects_final.csv'\n",
    "\n",
    "# Essential columns only (removes problematic text fields with quotes/commas)\n",
    "essential_cols = ['WDPA_PID', 'transectID', 'point_position', 'x', 'y']\n",
    "\n",
    "# Write first chunk with header\n",
    "print(\"Writing combined file with essential columns only...\")\n",
    "first_chunk = pd.read_csv(chunk_files[0], low_memory=False)[essential_cols]\n",
    "first_chunk.to_csv(output_file, index=False, mode='w')\n",
    "print(f\"  Wrote chunk 1/{len(chunk_files)}\")\n",
    "\n",
    "# Append remaining chunks without header\n",
    "for i, chunk_file in enumerate(chunk_files[1:], start=2):\n",
    "    chunk = pd.read_csv(chunk_file, low_memory=False)[essential_cols]\n",
    "    chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "    print(f\"  Wrote chunk {i}/{len(chunk_files)}\")\n",
    "    del chunk\n",
    "\n",
    "print(f\"\\nComplete! Saved to {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1024**2:.1f} MB\")\n",
    "\n",
    "# Now upload the combined CSV file as an asset to Earth Engine (x,y), crs is EPSG:4326\n",
    "# Uploading the asset takes ~40min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee-dev-kernel",
   "language": "python",
   "name": "gee-dev-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
