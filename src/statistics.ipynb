{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f821e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "data = pd.read_csv('/workspace/all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0be4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "\n",
    "# Rename columns\n",
    "data = data.rename(columns={\n",
    "    'gradient_mean': 'grad_mean',\n",
    "    'gradient_median': 'grad_med',\n",
    "    'gradient_stddev': 'grad_std',\n",
    "    'hm_stddev': 'hm_std',\n",
    "    'hm_median': 'hm_med'\n",
    "})\n",
    "\n",
    "# Pivot and calculate edge metrics\n",
    "zones_data = data.pivot_table(\n",
    "    index=['WDPA_PID', 'year'], \n",
    "    columns='zone',\n",
    "    values=['grad_mean', 'grad_med', 'hm_mean', 'hm_med']\n",
    ").reset_index()\n",
    "\n",
    "zones_data.columns = ['_'.join(col).strip('_') if col[1] else col[0] for col in zones_data.columns]\n",
    "\n",
    "# Edge level calculations\n",
    "zones_data['edge_level'] = zones_data['grad_mean_-1_1km'] / ((zones_data['grad_mean_-1_-3km'] + zones_data['grad_mean_1_3km']) / 2)\n",
    "zones_data['edge_level_far'] = zones_data['grad_mean_-1_1km'] / ((zones_data['grad_mean_-3_-5km'] + zones_data['grad_mean_3_5km']) / 2)\n",
    "zones_data['edge_level_med'] = zones_data['grad_med_-1_1km'] / ((zones_data['grad_med_-1_-3km'] + zones_data['grad_med_1_3km']) / 2)\n",
    "zones_data['edge_level_med_far'] = zones_data['grad_med_-1_1km'] / ((zones_data['grad_med_-3_-5km'] + zones_data['grad_med_3_5km']) / 2)\n",
    "\n",
    "# Calculate mean across all zones\n",
    "hm_mean_cols = [col for col in zones_data.columns if col.startswith('hm_mean_')]\n",
    "hm_med_cols = [col for col in zones_data.columns if col.startswith('hm_med_')]\n",
    "zones_data['hm_mean_all'] = zones_data[hm_mean_cols].mean(axis=1)\n",
    "zones_data['hm_med_all'] = zones_data[hm_med_cols].mean(axis=1)\n",
    "\n",
    "# Drop zone-specific columns\n",
    "cols_to_drop = [col for col in zones_data.columns if any(\n",
    "    col.startswith(prefix) for prefix in ['grad_mean_', 'grad_med_', 'grad_std_', 'hm_mean_', 'hm_med_', 'hm_std_']\n",
    ")]\n",
    "zones_data = zones_data.drop(columns=cols_to_drop)\n",
    "\n",
    "# Merge with other data\n",
    "other_cols = [col for col in data.columns if col not in ['zone', 'grad_mean', 'grad_med', 'grad_std', 'hm_mean', 'hm_med', 'hm_std']]\n",
    "df_other = data[other_cols].drop_duplicates(subset=['WDPA_PID', 'year'])\n",
    "\n",
    "df = zones_data.merge(df_other, on=['WDPA_PID', 'year'], how='left')\n",
    "\n",
    "# Add country data\n",
    "country = pd.read_csv(\"https://pkgstore.datahub.io/JohnSnowLabs/country-and-continent-codes-list/country-and-continent-codes-list-csv_csv/data/b7876b7f496677669644f3d1069d3121/country-and-continent-codes-list-csv_csv.csv\")\n",
    "country = country.rename(columns={'Three_Letter_Country_Code': 'ISO3'})\n",
    "df = df.merge(country, on='ISO3', how='left')\n",
    "\n",
    "print(f\"Loaded data: {len(df)} rows, {df['WDPA_PID'].nunique()} unique protected areas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c398c1e2",
   "metadata": {},
   "source": [
    "# Protected Area Edge Analysis\n",
    "\n",
    "This notebook analyzes edge effects in protected areas globally from 2001-2021, examining trends in vegetation gradient intensity at PA boundaries.\n",
    "\n",
    "## 1. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ced967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptivererere statistics\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Protected Areas: {df['WDPA_PID'].nunique():,}\")\n",
    "print(f\"Total Countries: {df['ISO3'].nunique()}\")\n",
    "print(f\"Total Biomes: {df['biome'].nunique()}\")\n",
    "print(f\"Time Period: {df['year'].min()}-{df['year'].max()}\")\n",
    "print(f\"\\nPAs by Biome:\\n{df.groupby('biome')['WDPA_PID'].nunique().to_string()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROTECTED AREA SIZE DISTRIBUTION (km²)\")\n",
    "print(\"=\"*60)\n",
    "area_km2 = df['AREA_DISSO'] / 1e6\n",
    "print(f\"Min: {area_km2.min():.2f} km²\")\n",
    "print(f\"25th percentile: {area_km2.quantile(0.25):.2f} km²\")\n",
    "print(f\"Median: {area_km2.median():.2f} km²\")\n",
    "print(f\"75th percentile: {area_km2.quantile(0.75):.2f} km²\")\n",
    "print(f\"Max: {area_km2.max():.2f} km²\")\n",
    "print(f\"Std Dev: {area_km2.std():.2f} km²\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EDGE LEVEL METRICS (2020)\")\n",
    "print(\"=\"*60)\n",
    "df_2020 = df[df['year'] == 2020]\n",
    "print(f\"Mean Edge Level (near): {df_2020['edge_level'].mean():.4f}\")\n",
    "print(f\"Mean Edge Level (far): {df_2020['edge_level_far'].mean():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHANGE IN EDGE LEVEL (2001-2020)\")\n",
    "print(\"=\"*60)\n",
    "df_2001 = df[df['year'] == 2001]\n",
    "change_near = df_2020['edge_level'].mean() - df_2001['edge_level'].mean()\n",
    "change_far = df_2020['edge_level_far'].mean() - df_2001['edge_level_far'].mean()\n",
    "print(f\"Change (near): {change_near:+.4f}\")\n",
    "print(f\"Change (far): {change_far:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef0c88",
   "metadata": {},
   "source": [
    "## 2. Temporal Trends Analysis\n",
    "\n",
    "Calculate linear regression slopes for each protected area to quantify edge level trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate temporal trends for each PA\n",
    "trends_list = []\n",
    "for ID in df['WDPA_PID'].unique():\n",
    "    df_pa = df[df['WDPA_PID'] == ID]\n",
    "    X = df_pa['year'].values\n",
    "    \n",
    "    # Model 1: edge_level ~ year\n",
    "    slope1, _, _, p1, se1 = stats.linregress(X, df_pa['edge_level'].values)\n",
    "    \n",
    "    # Model 2: edge_level_far ~ year\n",
    "    slope2, _, _, p2, se2 = stats.linregress(X, df_pa['edge_level_far'].values)\n",
    "    \n",
    "    trends_list.append({\n",
    "        'WDPA_PID': ID,\n",
    "        'edge_level_slope': slope1,\n",
    "        'edge_level_stderr': se1,\n",
    "        'edge_level_tvalue': slope1 / se1,\n",
    "        'edge_level_pvalue': p1,\n",
    "        'edge_level_far_slope': slope2,\n",
    "        'edge_level_far_stderr': se2,\n",
    "        'edge_level_far_tvalue': slope2 / se2,\n",
    "        'edge_level_far_pvalue': p2\n",
    "    })\n",
    "\n",
    "trends = pd.DataFrame(trends_list)\n",
    "df = df.merge(trends, on='WDPA_PID', how='left')\n",
    "\n",
    "# Classify trends\n",
    "def classify_trend(slope, pvalue):\n",
    "    if pvalue >= 0.05:\n",
    "        return \"Not Significant Increase\" if slope > 0 else \"Not Significant Decrease\"\n",
    "    return \"Significantly Increase\" if slope > 0 else \"Significantly Decrease\"\n",
    "\n",
    "df['edge_trend'] = df.apply(lambda row: classify_trend(row['edge_level_slope'], row['edge_level_pvalue']), axis=1)\n",
    "df['edge_trend_far'] = df.apply(lambda row: classify_trend(row['edge_level_far_slope'], row['edge_level_far_pvalue']), axis=1)\n",
    "\n",
    "# Overall trend statistics\n",
    "print(\"=\"*60)\n",
    "print(\"OVERALL TRENDS (all PAs)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean edge_level slope: {trends['edge_level_slope'].mean():.6f}\")\n",
    "t_stat1, p_val1 = stats.ttest_1samp(trends['edge_level_slope'].dropna(), 0)\n",
    "print(f\"  t-statistic: {t_stat1:.4f}, p-value: {p_val1:.2e}\")\n",
    "\n",
    "print(f\"\\nMean edge_level_far slope: {trends['edge_level_far_slope'].mean():.6f}\")\n",
    "t_stat2, p_val2 = stats.ttest_1samp(trends['edge_level_far_slope'].dropna(), 0)\n",
    "print(f\"  t-statistic: {t_stat2:.4f}, p-value: {p_val2:.2e}\")\n",
    "\n",
    "# Trend distribution\n",
    "edge_trend_counts = df[['WDPA_PID', 'edge_trend']].drop_duplicates()['edge_trend'].value_counts()\n",
    "total_PAs = df['WDPA_PID'].nunique()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TREND CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "for trend, count in edge_trend_counts.items():\n",
    "    print(f\"{trend}: {count} PAs ({count/total_PAs*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nSignificantly increasing: {edge_trend_counts.get('Significantly Increase', 0)/total_PAs*100:.1f}%\")\n",
    "print(f\"Any increase (sig + non-sig): {(edge_trend_counts.get('Significantly Increase', 0) + edge_trend_counts.get('Not Significant Increase', 0))/total_PAs*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9265e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Helper Functions for Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_anova_analysis(df, response_var, group_var, filter_invalid=None, title=\"\"):\n",
    "    \"\"\"\n",
    "    Run one-way ANOVA with Tukey HSD post-hoc test and normality check.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataframe\n",
    "    response_var : str\n",
    "        Name of response variable (e.g., 'edge_level_slope')\n",
    "    group_var : str\n",
    "        Name of grouping variable (e.g., 'IUCN_CAT')\n",
    "    filter_invalid : list, optional\n",
    "        List of values to exclude from group_var\n",
    "    title : str\n",
    "        Description for output\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with F-statistic, p-value, and Tukey results\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    analysis_data = df[['WDPA_PID', group_var, response_var]].drop_duplicates()\n",
    "    \n",
    "    if filter_invalid:\n",
    "        analysis_data = analysis_data[~analysis_data[group_var].isin(filter_invalid)]\n",
    "    \n",
    "    analysis_data = analysis_data.dropna(subset=[group_var, response_var])\n",
    "    \n",
    "    # Run ANOVA\n",
    "    groups = [group[response_var].values for name, group in analysis_data.groupby(group_var)]\n",
    "    f_stat, p_value = stats.f_oneway(*groups)\n",
    "    \n",
    "    # Tukey HSD\n",
    "    tukey = pairwise_tukeyhsd(endog=analysis_data[response_var], \n",
    "                              groups=analysis_data[group_var], \n",
    "                              alpha=0.05)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{title}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ANOVA: {response_var} ~ {group_var}\")\n",
    "    print(f\"  N = {len(analysis_data)} PAs\")\n",
    "    print(f\"  F-statistic = {f_stat:.4f}\")\n",
    "    print(f\"  p-value = {p_value:.2e}\")\n",
    "    print(f\"\\n{tukey}\")\n",
    "    print()\n",
    "    \n",
    "    return {'f_stat': f_stat, 'p_value': p_value, 'tukey': tukey, 'data': analysis_data}\n",
    "\n",
    "\n",
    "def run_linear_regression(df, response_var, predictor_var, log_transform=False, title=\"\"):\n",
    "    \"\"\"\n",
    "    Run OLS linear regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataframe\n",
    "    response_var : str\n",
    "        Name of response variable\n",
    "    predictor_var : str\n",
    "        Name of predictor variable\n",
    "    log_transform : bool\n",
    "        Whether to log-transform predictor\n",
    "    title : str\n",
    "        Description for output\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Fitted model object\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    analysis_data = df[['WDPA_PID', predictor_var, response_var]].drop_duplicates()\n",
    "    analysis_data = analysis_data.dropna(subset=[predictor_var, response_var])\n",
    "    \n",
    "    # Setup regression\n",
    "    X = np.log(analysis_data[predictor_var]) if log_transform else analysis_data[predictor_var]\n",
    "    X = sm.add_constant(X)\n",
    "    y = analysis_data[response_var]\n",
    "    \n",
    "    # Fit model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{title}\")\n",
    "    print(\"=\"*60)\n",
    "    predictor_str = f\"log({predictor_var})\" if log_transform else predictor_var\n",
    "    print(f\"Linear Regression: {response_var} ~ {predictor_str}\")\n",
    "    print(f\"  N = {len(analysis_data)} PAs\")\n",
    "    print(model.summary())\n",
    "    print()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_qq_residuals(analysis_data, response_var, group_var, title=\"\"):\n",
    "    \"\"\"\n",
    "    Create Q-Q plot for residuals to check normality assumption.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    analysis_data : DataFrame\n",
    "        Data from ANOVA analysis\n",
    "    response_var : str\n",
    "        Name of response variable\n",
    "    group_var : str\n",
    "        Name of grouping variable\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    residuals = analysis_data[response_var] - analysis_data.groupby(group_var)[response_var].transform('mean')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=ax)\n",
    "    ax.set_title(f'Q-Q Plot: {title}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e9b30",
   "metadata": {},
   "source": [
    "## 4. Factors Affecting Edge Trends\n",
    "\n",
    "Analyze how edge trends vary by IUCN category, protected area size, biome, and establishment year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ad2f7",
   "metadata": {},
   "source": [
    "### 4.1 IUCN Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IUCN Category Analysis\n",
    "invalid_iucn = ['Not Reported', 'Not Assigned', 'Not Applicable']\n",
    "\n",
    "# Near edge trends\n",
    "anova_iucn_near = run_anova_analysis(\n",
    "    df, \n",
    "    response_var='edge_level_slope',\n",
    "    group_var='IUCN_CAT',\n",
    "    filter_invalid=invalid_iucn,\n",
    "    title=\"IUCN CATEGORY - Near Edge Trends\"\n",
    ")\n",
    "\n",
    "# Far edge trends\n",
    "anova_iucn_far = run_anova_analysis(\n",
    "    df, \n",
    "    response_var='edge_level_far_slope',\n",
    "    group_var='IUCN_CAT',\n",
    "    filter_invalid=invalid_iucn,\n",
    "    title=\"IUCN CATEGORY - Far Edge Trends\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check normality of residuals\n",
    "plot_qq_residuals(anova_iucn_near['data'], 'edge_level_slope', 'IUCN_CAT', \n",
    "                  title=\"Near Edge Residuals by IUCN Category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26db194",
   "metadata": {},
   "source": [
    "### 4.2 Protected Area Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PA Size Analysis (log-transformed)\n",
    "\n",
    "# Near edge trends\n",
    "model_size_near = run_linear_regression(\n",
    "    df,\n",
    "    response_var='edge_level_slope',\n",
    "    predictor_var='AREA_DISSO',\n",
    "    log_transform=True,\n",
    "    title=\"PA SIZE - Near Edge Trends\"\n",
    ")\n",
    "\n",
    "# Far edge trends\n",
    "model_size_far = run_linear_regression(\n",
    "    df,\n",
    "    response_var='edge_level_far_slope',\n",
    "    predictor_var='AREA_DISSO',\n",
    "    log_transform=True,\n",
    "    title=\"PA SIZE - Far Edge Trends\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df1003",
   "metadata": {},
   "source": [
    "### 4.3 Biome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0757778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biome Analysis\n",
    "\n",
    "# Near edge trends\n",
    "anova_biome_near = run_anova_analysis(\n",
    "    df,\n",
    "    response_var='edge_level_slope',\n",
    "    group_var='biome',\n",
    "    title=\"BIOME - Near Edge Trends\"\n",
    ")\n",
    "\n",
    "# Far edge trends\n",
    "anova_biome_far = run_anova_analysis(\n",
    "    df,\n",
    "    response_var='edge_level_far_slope',\n",
    "    group_var='biome',\n",
    "    title=\"BIOME - Far Edge Trends\"\n",
    ")\n",
    "\n",
    "# Check normality\n",
    "plot_qq_residuals(anova_biome_near['data'], 'edge_level_slope', 'biome',\n",
    "                  title=\"Near Edge Residuals by Biome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3aa0a4",
   "metadata": {},
   "source": [
    "### 4.4 Establishment Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf45cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishment Year Analysis\n",
    "\n",
    "# Near edge trends\n",
    "model_year_near = run_linear_regression(\n",
    "    df,\n",
    "    response_var='edge_level_slope',\n",
    "    predictor_var='STATUS_YR',\n",
    "    log_transform=False,\n",
    "    title=\"ESTABLISHMENT YEAR - Near Edge Trends\"\n",
    ")\n",
    "\n",
    "# Far edge trends\n",
    "model_year_far = run_linear_regression(\n",
    "    df,\n",
    "    response_var='edge_level_far_slope',\n",
    "    predictor_var='STATUS_YR',\n",
    "    log_transform=False,\n",
    "    title=\"ESTABLISHMENT YEAR - Far Edge Trends\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75ef20",
   "metadata": {},
   "source": [
    "## 5. Mixed Effects Model\n",
    "\n",
    "Test the relationship between edge level, year, and human modification, accounting for repeated measures within protected areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed effects model: edge_level ~ year + hm_mean_all + (1|WDPA_PID)\n",
    "# This accounts for repeated measurements within each PA\n",
    "\n",
    "# Prepare data\n",
    "mixed_data = df[['WDPA_PID', 'year', 'edge_level', 'edge_level_far', 'hm_mean_all']].dropna()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MIXED EFFECTS MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"N observations: {len(mixed_data)}\")\n",
    "print(f\"N protected areas: {mixed_data['WDPA_PID'].nunique()}\")\n",
    "\n",
    "# Model 1: Near edge\n",
    "print(\"\\n--- Model 1: edge_level ~ year + hm_mean_all + (1|WDPA_PID) ---\")\n",
    "mixed_model_near = smf.mixedlm(\n",
    "    \"edge_level ~ year + hm_mean_all\", \n",
    "    data=mixed_data, \n",
    "    groups=mixed_data[\"WDPA_PID\"]\n",
    ").fit()\n",
    "print(mixed_model_near.summary())\n",
    "\n",
    "# Model 2: Far edge\n",
    "print(\"\\n--- Model 2: edge_level_far ~ year + hm_mean_all + (1|WDPA_PID) ---\")\n",
    "mixed_model_far = smf.mixedlm(\n",
    "    \"edge_level_far ~ year + hm_mean_all\", \n",
    "    data=mixed_data, \n",
    "    groups=mixed_data[\"WDPA_PID\"]\n",
    ").fit()\n",
    "print(mixed_model_far.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6b88a",
   "metadata": {},
   "source": [
    "## 6. Summary of Results\n",
    "\n",
    "Key findings from the statistical analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ac55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics table\n",
    "summary_results = {\n",
    "    'Analysis': [],\n",
    "    'Test': [],\n",
    "    'Statistic': [],\n",
    "    'P-value': [],\n",
    "    'Interpretation': []\n",
    "}\n",
    "\n",
    "# Overall trends\n",
    "summary_results['Analysis'].append('Overall Trend')\n",
    "summary_results['Test'].append('One-sample t-test')\n",
    "summary_results['Statistic'].append(f\"t={t_stat1:.2f}\")\n",
    "summary_results['P-value'].append(f\"{p_val1:.2e}\")\n",
    "summary_results['Interpretation'].append('Significant' if p_val1 < 0.05 else 'Not significant')\n",
    "\n",
    "# IUCN\n",
    "summary_results['Analysis'].append('IUCN Category')\n",
    "summary_results['Test'].append('ANOVA')\n",
    "summary_results['Statistic'].append(f\"F={anova_iucn_near['f_stat']:.2f}\")\n",
    "summary_results['P-value'].append(f\"{anova_iucn_near['p_value']:.2e}\")\n",
    "summary_results['Interpretation'].append('Trends vary by IUCN category' if anova_iucn_near['p_value'] < 0.05 else 'No difference')\n",
    "\n",
    "# Size\n",
    "summary_results['Analysis'].append('PA Size')\n",
    "summary_results['Test'].append('Linear Regression')\n",
    "summary_results['Statistic'].append(f\"β={model_size_near.params[1]:.4f}\")\n",
    "summary_results['P-value'].append(f\"{model_size_near.pvalues[1]:.2e}\")\n",
    "summary_results['Interpretation'].append('Trends vary with size' if model_size_near.pvalues[1] < 0.05 else 'No relationship')\n",
    "\n",
    "# Biome\n",
    "summary_results['Analysis'].append('Biome')\n",
    "summary_results['Test'].append('ANOVA')\n",
    "summary_results['Statistic'].append(f\"F={anova_biome_near['f_stat']:.2f}\")\n",
    "summary_results['P-value'].append(f\"{anova_biome_near['p_value']:.2e}\")\n",
    "summary_results['Interpretation'].append('Trends vary by biome' if anova_biome_near['p_value'] < 0.05 else 'No difference')\n",
    "\n",
    "# Establishment Year\n",
    "summary_results['Analysis'].append('Establishment Year')\n",
    "summary_results['Test'].append('Linear Regression')\n",
    "summary_results['Statistic'].append(f\"β={model_year_near.params[1]:.4f}\")\n",
    "summary_results['P-value'].append(f\"{model_year_near.pvalues[1]:.2e}\")\n",
    "summary_results['Interpretation'].append('Trends vary with year' if model_year_near.pvalues[1] < 0.05 else 'No relationship')\n",
    "\n",
    "# Mixed model\n",
    "summary_results['Analysis'].append('Year + Human Modification')\n",
    "summary_results['Test'].append('Mixed Effects Model')\n",
    "summary_results['Statistic'].append(f\"β_year={mixed_model_near.params['year']:.4f}\")\n",
    "summary_results['P-value'].append(f\"{mixed_model_near.pvalues['year']:.2e}\")\n",
    "summary_results['Interpretation'].append('Significant temporal effect' if mixed_model_near.pvalues['year'] < 0.05 else 'No temporal effect')\n",
    "\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY TABLE - NEAR EDGE LEVEL SLOPE ANALYSES\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620394f0",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed data with trends\n",
    "output_file = '/workspace/results/pa_edge_trends_analysis.csv'\n",
    "df_export = df[['WDPA_PID', 'year', 'edge_level', 'edge_level_far', \n",
    "                'edge_level_slope', 'edge_level_pvalue', 'edge_trend',\n",
    "                'edge_level_far_slope', 'edge_level_far_pvalue', 'edge_trend_far',\n",
    "                'IUCN_CAT', 'AREA_DISSO', 'STATUS_YR', 'biome', \n",
    "                'ISO3', 'Country_Name', 'Continent_Name',\n",
    "                'hm_mean_all', 'hm_med_all']].copy()\n",
    "\n",
    "df_export.to_csv(output_file, index=False)\n",
    "print(f\"Results exported to: {output_file}\")\n",
    "print(f\"Rows: {len(df_export)}, Columns: {len(df_export.columns)}\")\n",
    "\n",
    "# Export summary table\n",
    "summary_df.to_csv('/workspace/results/statistical_summary.csv', index=False)\n",
    "print(f\"Summary table exported to: /workspace/results/statistical_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
