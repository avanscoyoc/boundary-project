{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d56b83",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a062bd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ee/deprecation.py:209: DeprecationWarning: \n",
      "\n",
      "Attention required for MODIS/006/MOD09A1! You are using a deprecated asset.\n",
      "To make sure your code keeps working, please update it.\n",
      "Learn more: https://developers.google.com/earth-engine/datasets/catalog/MODIS_006_MOD09A1\n",
      "\n",
      "  warnings.warn(warning, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Remove the module from cache if it exists\n",
    "if 'utils2' in sys.modules:\n",
    "    del sys.modules['utils2']\n",
    "\n",
    "from utils2 import GeometryOperations\n",
    "\n",
    "import ee\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "ee.Authenticate()\n",
    "ee.Initialize(project='dse-staff')\n",
    "\n",
    "PROTECTED_AREAS = ee.FeatureCollection(\"WCMC/WDPA/202106/polygons\")\n",
    "ECOREGIONS = gpd.read_file(\"../data/Ecoregions2017/Ecoregions2017.shp\")\n",
    "WATER = ee.Image(\"JRC/GSW1_4/GlobalSurfaceWater\")\n",
    "HM_IMAGE = ee.ImageCollection('CSP/HM/GlobalHumanModification').mean()\n",
    "MODIS = ee.ImageCollection('MODIS/006/MOD09A1')\n",
    "\n",
    "geom_ops = GeometryOperations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_polygon = PROTECTED_AREAS.map(geom_ops.set_geometry_type) \\\n",
    "    .filter(geom_ops.get_pa_filter(\"Polygon\")) \\\n",
    "    .map(geom_ops.get_biome) #6358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdpa_pids_ee = filtered_polygon.aggregate_array('WDPA_PID')\n",
    "wdpaids = wdpa_pids_ee.getInfo()\n",
    "len(wdpaids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8752d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ee.batch.Export.table.toDrive(\n",
    "    collection=filtered_polygon,\n",
    "    description='WDPA_filtered_polygons',\n",
    "    fileFormat='SHP'\n",
    ")\n",
    "task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acace5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_crs = \"ESRI:54009\"\n",
    "wdpa = gpd.read_file('../data/WDPA_filtered_polygons.geojson').to_crs(target_crs)\n",
    "len(wdpa) #6358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4047415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill holes smaller than 1500m x 1500m\n",
    "\n",
    "filled = geom_ops.fill_holes(wdpa, max_hole_area=2250000)  # 1500m * 1500m = 2250000 sq meters\n",
    "print(f\"Number of polygons after filling holes: {len(filled)}\") #6358\n",
    "print(f\"Original polygon count: {len(wdpa)}\") #6358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d61200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find overlap groups\n",
    "overlap_groups = geom_ops.find_overlap_groups(wdpa, overlap_threshold=90)\n",
    "\n",
    "# Select best row from each group and create new dataset\n",
    "selected_rows = []\n",
    "for group_indices in overlap_groups:\n",
    "    if len(group_indices) == 1:\n",
    "        # Single geometry, keep as is\n",
    "        selected_rows.append(wdpa.loc[group_indices[0]])\n",
    "    else:\n",
    "        # Multiple overlapping geometries, select best one\n",
    "        group_df = wdpa.loc[group_indices]\n",
    "        best_row = geom_ops.get_min_year_from_group(group_df)\n",
    "        selected_rows.append(best_row)\n",
    "        print(f\"Overlap group of {len(group_indices)} geometries - selected WDPA_PID: {best_row['WDPA_PID']}\")\n",
    "\n",
    "# Create new GeoDataFrame with selected rows\n",
    "deduped_overlaps = gpd.GeoDataFrame(selected_rows, crs=wdpa.crs)\n",
    "\n",
    "print(f\"Original count: {len(wdpa)}\") #6358\n",
    "print(f\"After removing >90% overlaps: {len(deduped_overlaps)}\") #5629\n",
    "print(f\"Removed {len(wdpa) - len(deduped_overlaps)} overlapping geometries\") #729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate ORIG_NAME next\n",
    "print(\"Checking for duplicate ORIG_NAME...\")\n",
    "name_duplicates = deduped_overlaps['ORIG_NAME'].duplicated().sum()\n",
    "print(f\"Duplicate ORIG_NAME: {name_duplicates}\") #60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9629277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ORIG_NAME and apply the function\n",
    "grouped = deduped_overlaps.groupby('ORIG_NAME').apply(lambda x: geom_ops.get_min_year_from_group(x)).reset_index(drop=True)\n",
    "\n",
    "# Now dissolve the geometries while keeping the selected attributes\n",
    "dissolved = grouped.dissolve(by='ORIG_NAME', as_index=False)\n",
    "print(f\"Number of polygons after dissolving by ORIG_NAME: {len(dissolved)}\") #5569"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f8a1b",
   "metadata": {},
   "source": [
    "After deduping, finish filtering out narrow PAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove narrow polygons based on perimeter-to-area ratio\n",
    "# Recalculate area per geometry \n",
    "dissolved[\"AREA_DISSOLVED\"] = dissolved.geometry.area\n",
    "dissolved[\"PERIMETER\"] = dissolved.geometry.length  # length in CRS units\n",
    "dissolved[\"PA_RATIO\"]  = dissolved[\"PERIMETER\"] / dissolved[\"AREA_DISSOLVED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5993f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "q75 = dissolved[\"PA_RATIO\"].quantile(0.75) #0.00039173069233858557\n",
    "print(f\"75th percentile of PA_RATIO: {q75}\")\n",
    "check = dissolved[dissolved[\"PA_RATIO\"] >= q75]\n",
    "check.to_file(\"../data/q75.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fa399",
   "metadata": {},
   "outputs": [],
   "source": [
    "q90 = dissolved[\"PA_RATIO\"].quantile(0.90) #0.0005701899361271124\n",
    "print(f\"90th percentile of PA_RATIO: {q90}\")\n",
    "check2 = dissolved[dissolved[\"PA_RATIO\"] >= q90]\n",
    "check2.to_file(\"../data/q90.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdpa_final = dissolved[dissolved[\"PA_RATIO\"] < q90]\n",
    "len(wdpa_final['WDPA_PID']) #5012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b441a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdpa_final.to_file(\"../data/wdpa_final.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1639839",
   "metadata": {},
   "source": [
    "CREATE ZONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cdcfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdpa_final = gpd.read_file(\"../data/wdpa_final.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba68af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wdpa_final)  # 5012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827575ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_list = []\n",
    "\n",
    "for idx, park in wdpa_final.iterrows():\n",
    "    geom = park.geometry\n",
    "    \n",
    "    # Outer zones (rings extending outward from park boundary)\n",
    "    center = geom.buffer(1000).difference(geom.buffer(-1000))  # -1 to +1km \n",
    "    outer = geom.buffer(3000).difference(geom.buffer(1000))    # +1 to +3km\n",
    "    far_outer = geom.buffer(5000).difference(geom.buffer(3000)) # +3 to +5km\n",
    "    \n",
    "    # Inner zones (rings extending inward from park boundary)\n",
    "    inner = geom.buffer(-1000).difference(geom.buffer(-3000))   # -1 to -3km (ring inside park)\n",
    "    far_inner = geom.buffer(-3000).difference(geom.buffer(-5000)) # -3 to -5km (ring inside park)\n",
    "\n",
    "    base_props = {col: park[col] for col in park.index if col not in ['geometry', 'geometry_t']}\n",
    "    \n",
    "    zones_list.extend([\n",
    "        {**base_props, 'zone': '-1_1km', 'geometry': center},\n",
    "        {**base_props, 'zone': '1_3km', 'geometry': outer},\n",
    "        {**base_props, 'zone': '3_5km', 'geometry': far_outer},\n",
    "        {**base_props, 'zone': '-1_-3km', 'geometry': inner},\n",
    "        {**base_props, 'zone': '-3_-5km', 'geometry': far_inner}\n",
    "    ])\n",
    "\n",
    "zones = gpd.GeoDataFrame(zones_list, crs=wdpa_final.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5683a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to just the 3_5km zones\n",
    "zones_3_5 = zones[zones['zone'] == '3_5km'].copy()\n",
    "\n",
    "# Use spatial index for efficient intersection checking\n",
    "sindex = zones_3_5.sindex\n",
    "\n",
    "shared_border = []\n",
    "for idx, zone in zones_3_5.iterrows():\n",
    "    # Find potential intersections using spatial index\n",
    "    possible_matches_idx = list(sindex.intersection(zone.geometry.bounds))\n",
    "    possible_matches = zones_3_5.iloc[possible_matches_idx]\n",
    "    \n",
    "    # Check if any other zone (different WDPA_PID) intersects\n",
    "    has_border = False\n",
    "    for other_idx, other_zone in possible_matches.iterrows():\n",
    "        if (idx != other_idx and \n",
    "            zone['WDPA_PID'] != other_zone['WDPA_PID'] and\n",
    "            zone.geometry.intersects(other_zone.geometry)):\n",
    "            has_border = True\n",
    "            break\n",
    "    \n",
    "    shared_border.append(has_border)\n",
    "\n",
    "zones_3_5['SHARED_BORDER'] = shared_border\n",
    "\n",
    "# Merge back to full zones dataset if needed\n",
    "zones = zones.merge(\n",
    "    zones_3_5[['WDPA_PID', 'zone', 'SHARED_BORDER']], \n",
    "    on=['WDPA_PID', 'zone'], \n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9379c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones.to_file('/workspace/data/zones/zones.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec9f915",
   "metadata": {},
   "source": [
    "RUN ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d6a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = ee.FeatureCollection('projects/dse-staff/assets/zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f2dd1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## More efficient approach: Calculate water percentage with reduceRegions (batch processing)\n",
    "#permanent_water = WATER.select('occurrence').gte(90)\n",
    "#\n",
    "## Create a binary image: 1 for water, 0 for land\n",
    "#water_binary = permanent_water.unmask(0)\n",
    "#\n",
    "## Use reduceRegions to calculate mean (which gives % water when binary)\n",
    "#zones_with_water = water_binary.reduceRegions(\n",
    "#    collection=zones,\n",
    "#    reducer=ee.Reducer.mean().setOutputs(['perc_water']),\n",
    "#    scale=30,\n",
    "#    tileScale=4\n",
    "#).map(lambda f: f.set('perc_water', ee.Number(f.get('perc_water')).multiply(100)))\n",
    "\n",
    "# Now use this for HM calculation\n",
    "hm_results = HM_IMAGE.reduceRegions(\n",
    "    collection=zones,\n",
    "    reducer=ee.Reducer.mean().combine(ee.Reducer.stdDev(), '', True).combine(ee.Reducer.median(), '', True)\n",
    "        .setOutputs(['hm_mean', 'hm_std', 'hm_med']),\n",
    "    scale=500,\n",
    "    tileScale=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f2e4b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year 2001 (1/21)...\n",
      "Export task started: G3VP6W32FPCVQPTK74UHV757 for 2001\n",
      "Processing year 2002 (2/21)...\n",
      "Export task started: SYAZUGFJEJI4FRNEGMC64BBO for 2002\n",
      "Processing year 2003 (3/21)...\n",
      "Export task started: E72OQEK3O5SJUSCHJRV4DHZE for 2003\n",
      "Processing year 2004 (4/21)...\n",
      "Export task started: V23LHWETVIN74KYEHEIY3APY for 2004\n",
      "Processing year 2005 (5/21)...\n",
      "Export task started: M5PUHDVVFUOT3HDSKZDQ7QMD for 2005\n",
      "Processing year 2006 (6/21)...\n",
      "Export task started: B6NAKOQSXH2L6CNF4U5BR2NX for 2006\n",
      "Processing year 2007 (7/21)...\n",
      "Export task started: JQ5SRQHHP3NSLSBJFI6UA53Q for 2007\n",
      "Processing year 2008 (8/21)...\n",
      "Export task started: DO3AQ75Y6IBOQPRQFQ537FKJ for 2008\n",
      "Processing year 2009 (9/21)...\n",
      "Export task started: FXZ5BIW6KSAWJ3LKSM3P2ALZ for 2009\n",
      "Processing year 2010 (10/21)...\n",
      "Export task started: ZJZQUL7NSGA4BEP45WFZDCFL for 2010\n",
      "Processing year 2011 (11/21)...\n",
      "Export task started: K423FW75PX3FMFORBU376GKA for 2011\n",
      "Processing year 2012 (12/21)...\n",
      "Export task started: 4XBL7UJR5BD7Q6JNHXYTH3TZ for 2012\n",
      "Processing year 2013 (13/21)...\n",
      "Export task started: OU5NJWSRODDAPQTY2THZ4HVI for 2013\n",
      "Processing year 2014 (14/21)...\n",
      "Export task started: VOFO45QNIEWDGH3ESMMSVPTI for 2014\n",
      "Processing year 2015 (15/21)...\n",
      "Export task started: XMBKTWZOH3RRTFQ7KFWGTIS7 for 2015\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Task 2002 COMPLETED\n",
      "Processing year 2016 (16/21)...\n",
      "Export task started: VWVEB5HOWYKEE3OVFYDVBLJJ for 2016\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Task 2003 COMPLETED\n",
      "Processing year 2017 (17/21)...\n",
      "Export task started: EDQQIUHVXN77FCGDJ3ZN4RZV for 2017\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Task 2001 COMPLETED\n",
      "Processing year 2018 (18/21)...\n",
      "Export task started: VDFLFEMP2GU6UFZWHNM6GZIZ for 2018\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Task 2004 COMPLETED\n",
      "Processing year 2019 (19/21)...\n",
      "Export task started: 6YV77HO753JHFN3IY3XY5PH2 for 2019\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Task 2005 COMPLETED\n",
      "Processing year 2020 (20/21)...\n",
      "Export task started: SGHXVDKY2SNVBWYM24DF4AVG for 2020\n",
      "Waiting... 15 tasks active\n",
      "Waiting... 15 tasks active\n",
      "Task 2006 COMPLETED\n",
      "Processing year 2021 (21/21)...\n",
      "Export task started: 47NDFIVROLP5HAOC544OIGFQ for 2021\n",
      "Waiting for remaining tasks to complete...\n",
      "Still waiting for 15 tasks...\n",
      "Still waiting for 15 tasks...\n",
      "Still waiting for 15 tasks...\n",
      "Still waiting for 15 tasks...\n",
      "Still waiting for 15 tasks...\n",
      "Still waiting for 15 tasks...\n",
      "Still waiting for 15 tasks...\n",
      "Task 2007 COMPLETED\n",
      "Still waiting for 14 tasks...\n",
      "Still waiting for 14 tasks...\n",
      "Still waiting for 14 tasks...\n",
      "Still waiting for 14 tasks...\n",
      "Still waiting for 14 tasks...\n",
      "Still waiting for 14 tasks...\n",
      "Still waiting for 14 tasks...\n",
      "Still waiting for 14 tasks...\n",
      "Still waiting for 14 tasks...\n",
      "Still waiting for 14 tasks...\n",
      "Task 2009 COMPLETED\n",
      "Still waiting for 13 tasks...\n",
      "Still waiting for 13 tasks...\n",
      "Still waiting for 13 tasks...\n",
      "Still waiting for 13 tasks...\n",
      "Still waiting for 13 tasks...\n",
      "Task 2008 COMPLETED\n",
      "Still waiting for 12 tasks...\n",
      "Still waiting for 12 tasks...\n",
      "Still waiting for 12 tasks...\n",
      "Still waiting for 12 tasks...\n",
      "Still waiting for 12 tasks...\n",
      "Still waiting for 12 tasks...\n",
      "Still waiting for 12 tasks...\n",
      "Still waiting for 12 tasks...\n",
      "Still waiting for 12 tasks...\n",
      "Still waiting for 12 tasks...\n",
      "Still waiting for 12 tasks...\n",
      "Still waiting for 12 tasks...\n",
      "Task 2010 COMPLETED\n",
      "Still waiting for 11 tasks...\n",
      "Still waiting for 11 tasks...\n",
      "Still waiting for 11 tasks...\n",
      "Task 2011 COMPLETED\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Still waiting for 10 tasks...\n",
      "Task 2013 COMPLETED\n",
      "Still waiting for 9 tasks...\n",
      "Still waiting for 9 tasks...\n",
      "Still waiting for 9 tasks...\n",
      "Still waiting for 9 tasks...\n",
      "Still waiting for 9 tasks...\n",
      "Still waiting for 9 tasks...\n",
      "Still waiting for 9 tasks...\n",
      "Still waiting for 9 tasks...\n",
      "Still waiting for 9 tasks...\n",
      "Still waiting for 9 tasks...\n",
      "Still waiting for 9 tasks...\n",
      "Task 2014 COMPLETED\n",
      "Still waiting for 8 tasks...\n",
      "Still waiting for 8 tasks...\n",
      "Still waiting for 8 tasks...\n",
      "Still waiting for 8 tasks...\n",
      "Still waiting for 8 tasks...\n",
      "Still waiting for 8 tasks...\n",
      "Still waiting for 8 tasks...\n",
      "Task 2012 COMPLETED\n",
      "Still waiting for 7 tasks...\n",
      "Still waiting for 7 tasks...\n",
      "Still waiting for 7 tasks...\n",
      "Still waiting for 7 tasks...\n",
      "Task 2015 COMPLETED\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Still waiting for 6 tasks...\n",
      "Task 2017 COMPLETED\n",
      "Still waiting for 5 tasks...\n",
      "Still waiting for 5 tasks...\n",
      "Still waiting for 5 tasks...\n",
      "Task 2016 COMPLETED\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Still waiting for 4 tasks...\n",
      "Task 2019 COMPLETED\n",
      "Still waiting for 3 tasks...\n",
      "Still waiting for 3 tasks...\n",
      "Task 2018 COMPLETED\n",
      "Still waiting for 2 tasks...\n",
      "Still waiting for 2 tasks...\n",
      "Still waiting for 2 tasks...\n",
      "Still waiting for 2 tasks...\n",
      "Still waiting for 2 tasks...\n",
      "Still waiting for 2 tasks...\n",
      "Still waiting for 2 tasks...\n",
      "Still waiting for 2 tasks...\n",
      "Task 2021 COMPLETED\n",
      "Still waiting for 1 tasks...\n",
      "Still waiting for 1 tasks...\n",
      "Still waiting for 1 tasks...\n",
      "Still waiting for 1 tasks...\n",
      "Still waiting for 1 tasks...\n",
      "Still waiting for 1 tasks...\n",
      "Still waiting for 1 tasks...\n",
      "Still waiting for 1 tasks...\n",
      "Still waiting for 1 tasks...\n",
      "Still waiting for 1 tasks...\n",
      "Still waiting for 1 tasks...\n",
      "Still waiting for 1 tasks...\n",
      "Task 2020 COMPLETED\n",
      "All export tasks completed!\n"
     ]
    }
   ],
   "source": [
    "# Process multiple years of MODIS gradient data with task queue management\n",
    "import time\n",
    "\n",
    "years = list(range(2001, 2022)) \n",
    "max_concurrent_tasks = 15\n",
    "submitted_tasks = []\n",
    "\n",
    "def check_task_status():\n",
    "    \"\"\"Check status of submitted tasks and remove completed ones\"\"\"\n",
    "    global submitted_tasks\n",
    "    active_tasks = []\n",
    "    for task_obj, year in submitted_tasks:\n",
    "        task_status = task_obj.status()\n",
    "        if task_status['state'] in ['COMPLETED', 'FAILED', 'CANCELLED']:\n",
    "            print(f\"Task {year} {task_status['state']}\")\n",
    "        else:\n",
    "            active_tasks.append((task_obj, year))\n",
    "    submitted_tasks = active_tasks\n",
    "    return len(submitted_tasks)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    # Wait if we have too many active tasks\n",
    "    while check_task_status() >= max_concurrent_tasks:\n",
    "        print(f\"Waiting... {len(submitted_tasks)} tasks active\")\n",
    "        time.sleep(30)  # Check every 30 seconds\n",
    "    \n",
    "    print(f\"Processing year {year} ({i+1}/{len(years)})...\")\n",
    "    \n",
    "    # Get MODIS and calculate NDVI\n",
    "    modis = MODIS.filterDate(f'{year}-01-01', f'{year}-12-31') \\\n",
    "        .max() \\\n",
    "        .select(['sur_refl_b01', 'sur_refl_b02','sur_refl_b03','sur_refl_b04','sur_refl_b05','sur_refl_b06','sur_refl_b07'])  \n",
    "    \n",
    "    # Calculate NDVI\n",
    "    #ndvi = modis.normalizedDifference(['sur_refl_b02', 'sur_refl_b01']).rename('ndvi').select('ndvi')\n",
    "    \n",
    "    # Calculate gradient \n",
    "    grad = modis.gradient()\n",
    "    magnitude = grad.expression('sqrt(x*x + y*y)', {'x': grad.select('x'), 'y': grad.select('y')})\n",
    "\n",
    "    # Reduce with explicit CRS and scale matching MODIS\n",
    "    final_results = magnitude.reduceRegions(\n",
    "        collection=hm_results,\n",
    "        reducer=ee.Reducer.mean().combine(ee.Reducer.stdDev(), '', True).combine(ee.Reducer.median(), '', True)\n",
    "            .setOutputs(['grad_mean', 'grad_std', 'grad_med']),\n",
    "        scale=500,   \n",
    "        tileScale=8\n",
    "    )\n",
    "    \n",
    "    # Export results and track task\n",
    "    export_task = ee.batch.Export.table.toDrive(\n",
    "        collection=final_results,\n",
    "        description=f'final_results_{year}',\n",
    "        folder='pa_results_20251112',  # This will create/use a 'results' folder in your Google Drive\n",
    "        fileNamePrefix=f'final_results_{year}',\n",
    "        fileFormat='CSV',\n",
    "        selectors=['WDPA_PID', 'ORIG_NAME', 'GOV_TYPE', 'OWN_TYPE',\n",
    "        'STATUS_YR', 'IUCN_CAT', 'GIS_AREA', 'CONS_OBJ', 'DESIG', 'DESIG_ENG',\n",
    "        'DESIG_TYPE', 'GIS_M_AREA', 'INT_CRIT', 'ISO3', 'MANG_AUTH',\n",
    "        'MANG_PLAN', 'MARINE', 'METADATAID', 'NAME', 'NO_TAKE',\n",
    "        'NO_TK_AREA', 'PARENT_ISO', 'PA_DEF', 'REP_AREA', 'REP_M_AREA',\n",
    "        'STATUS', 'SUB_LOC', 'SUPP_INFO', 'VERIF', 'WDPAID','BIOME_NAME',\n",
    "        'PA_RATIO', 'AREA_DISSO', 'PERIMETER', 'SHARED_BOR', \n",
    "        'zone',\n",
    "        'hm_mean', 'hm_std', 'hm_med', \n",
    "        'grad_mean', 'grad_std', 'grad_med']\n",
    "    )\n",
    "    export_task.start()\n",
    "    submitted_tasks.append((export_task, year))\n",
    "    print(f\"Export task started: {export_task.id} for {year}\")\n",
    "\n",
    "# Wait for remaining tasks to complete\n",
    "print(\"Waiting for remaining tasks to complete...\")\n",
    "while check_task_status() > 0:\n",
    "    print(f\"Still waiting for {len(submitted_tasks)} tasks...\")\n",
    "    time.sleep(30)\n",
    "\n",
    "print(\"All export tasks completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
